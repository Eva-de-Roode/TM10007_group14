{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiDn2Sk-VWqE",
        "outputId": "7d2a6d8b-4e85-46a7-f99e-cd70d0749723"
      },
      "outputs": [],
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [],
      "source": [
        "#general packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets as ds\n",
        "import seaborn\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "# Classifiers\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn import feature_selection\n",
        "from sklearn import preprocessing\n",
        "from sklearn import neighbors\n",
        "from sklearn import svm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b_-bnn1_nBp"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NE_fTbKGe5z",
        "outputId": "29ed0236-10b5-48f4-fad9-d8a9a4213aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of samples: 186\n",
            "The number of columns: 494\n"
          ]
        }
      ],
      "source": [
        "# Data loading functions. Uncomment the one you want to use\n",
        "import pandas as pd\n",
        "from worcliver.load_data import load_data\n",
        "\n",
        "\n",
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dividing our data into test-, training- and validation-set\n",
        "\n",
        "The dataset should be randomly divided into :\n",
        "* training datasets = 70 % of samples\n",
        "* test datasets = 20 % of samples\n",
        "* validation datasets = 10 % of samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training: 130 samples, Test: 37 samples, Validation: 19 samples.\n",
            "So total number of samples: 186\n"
          ]
        }
      ],
      "source": [
        "amount_in_training = round(len(data.index)*0.7)\n",
        "amount_in_test = round(len(data.index)*0.2)\n",
        "amount_in_validation = len(data.index) - amount_in_training - amount_in_test\n",
        "\n",
        "#split the data into training data and test+validation data\n",
        "training_data, test_validation_data = model_selection.train_test_split(data, test_size=(amount_in_test + amount_in_validation) / len(data), random_state=42)\n",
        "\n",
        "#split the test+validation data into seperate sets for test and validation\n",
        "test_data, validation_data = model_selection.train_test_split(test_validation_data, test_size=amount_in_validation / (amount_in_test + amount_in_validation), random_state=42)\n",
        "\n",
        "#checking the lengths\n",
        "print(f'Training: {len(training_data)} samples, Test: {len(test_data)} samples, Validation: {len(validation_data)} samples.')\n",
        "print(f'So total number of samples: {len(training_data)+len(test_data)+len(validation_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracting the label per patient\n",
        "\n",
        "This label (benign/malignant) is the ground truth per patient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7ONVSyb_nBr",
        "outputId": "97d0a174-32d3-4270-81f2-00982ea8ec4f"
      },
      "outputs": [],
      "source": [
        "label_training = training_data[['label']]\n",
        "training_data.drop(columns=['label'], inplace=True)\n",
        "\n",
        "label_training_binary = label_training.copy()\n",
        "label_training_binary['label'] = label_training_binary['label'].map({'malignant': 1, 'benign': 0})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Finding the missing data\n",
        "\n",
        "Checking whether the missing data is stored as NaN or 0, finding the columns in which missing data is found as well as the empty features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of features in which there is missing data: 29\n",
            "The number of missing data is: 3046\n"
          ]
        }
      ],
      "source": [
        "#making a dataframe\n",
        "data_training = pd.DataFrame(training_data)\n",
        "\n",
        "#checking whether missing data is stored as NaN\n",
        "nan_check = data_training.isna()\n",
        "\n",
        "#conclusion: missing data is not stored as NaN\n",
        "\n",
        "#checking whether missing data is stored as 0\n",
        "zero_counts = (data_training == 0).sum()\n",
        "\n",
        "#conclusion: missing data is stored as 0\n",
        "\n",
        "zero_counts_columns = zero_counts[zero_counts > 0]\n",
        "print(f'The number of features in which there is missing data: {zero_counts_columns.count()}')\n",
        "print(f'The number of missing data is: {sum(zero_counts_columns)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Handling the missing data\n",
        "\n",
        "The missing data was handled as follows:\n",
        "1. Deleting the empty features from the data\n",
        "2. Replacing the missing data with the median value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The amount of data missing in PREDICT_original_tf_LBP_quartile_range_R8_P24 is 31\n",
            "The amount of data missing in PREDICT_original_phasef_monogenic_peak_WL3_N5 is 3\n",
            "The amount of data missing in PREDICT_original_phasef_monogenic_peak_position_WL3_N5 is 3\n",
            "The amount of data missing in PREDICT_original_phasef_phasecong_quartile_range_WL3_N5 is 22\n",
            "The amount of data missing in PREDICT_original_phasef_phasesym_quartile_range_WL3_N5 is 47\n",
            "Empty features: 24, Features with missing data: 5.\n"
          ]
        }
      ],
      "source": [
        "#step 1: deleting the empty features from the data\n",
        "#a feature is empty when it contains values for <30% of the samples\n",
        "#   empty_features = the features that are considered empty\n",
        "#   missing_data_features = the features that contain missing data\n",
        "empty_features = []\n",
        "missing_data_features = []\n",
        "\n",
        "for feature, missing_data in zero_counts_columns.items():  \n",
        "    if missing_data > 0.7*len(data_training.index):\n",
        "        empty_features.append(feature)\n",
        "    else:\n",
        "        missing_data_features.append(feature)\n",
        "        print(f'The amount of data missing in {feature} is {missing_data}')\n",
        "\n",
        "data_training_clean = data_training.drop(columns=empty_features)\n",
        "\n",
        "print(f'Empty features: {len(empty_features)}, Features with missing data: {len(missing_data_features)}.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To check: the number of features in which there is missing data is now: 0\n"
          ]
        }
      ],
      "source": [
        "#step 2: replacing the missing data with the median\n",
        "for feature in missing_data_features:\n",
        "    data_training_clean[feature].replace(0, np.nan, inplace=True)\n",
        "    \n",
        "    # Vervang de NaN waarden door de mediaan\n",
        "    median_value = data_training_clean[feature].median()\n",
        "    data_training_clean[feature].fillna(median_value, inplace=True)\n",
        "\n",
        "check_zero_counts = (data_training_clean == 0).sum()\n",
        "check_zero_counts_columns = check_zero_counts[check_zero_counts > 0]\n",
        "print(f'To check: the number of features in which there is missing data is now: {check_zero_counts_columns.count()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pre-processing: standardization/normalization\n",
        "\n",
        "First is tested whether the data is normally distributed or not with the Shapiro-Wilk Test. \n",
        "* p-value > 0.05: the data is likely normally distributed\n",
        "* p-value < 0.05: the data is likely not normally distributed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of features that do not have variation in their values is: 10\n",
            "The number of normally distributed features is: 53\n"
          ]
        }
      ],
      "source": [
        "shapiro_results = {}\n",
        "no_variation = 0\n",
        "features_no_variation = []\n",
        "\n",
        "for feature in data_training_clean.columns:\n",
        "    if data_training_clean[feature].max() == data_training_clean[feature].min():\n",
        "        shapiro_results[feature] = None  #skipping the feature when there is no variation                   WILLEN WE DEZE FEATURE DAN OOK WISSEN?\n",
        "        no_variation += 1\n",
        "        features_no_variation.append(feature)\n",
        "    else:\n",
        "        #Shapiro-Wilk test to check for normality\n",
        "        stat, p_value = stats.shapiro(data_training_clean[feature])\n",
        "\n",
        "        #if normally distributed, store 1, else store 0\n",
        "        if p_value > 0.05:\n",
        "            result = 1\n",
        "        else:\n",
        "            result = 0\n",
        "\n",
        "        shapiro_results[feature] = result\n",
        "\n",
        "#the number of normally distributed features\n",
        "print(f'The number of features that do not have variation in their values is: {no_variation}')\n",
        "print(f'The number of normally distributed features is: {sum(1 for result in shapiro_results.values() if result == 1)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#code from workgroup; sklearn package has a function that standardizes the data, so this is applied here:\n",
        "scaler = preprocessing.StandardScaler()\n",
        "data_training_standardized = scaler.fit_transform(data_training_clean)\n",
        "data_training_standardized_df = pd.DataFrame(data_training_standardized, columns=data_training_clean.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
