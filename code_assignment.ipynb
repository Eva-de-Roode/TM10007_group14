{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31ae3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0731fc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training + validation: 149 samples, Test: 37 samples.\n",
      "So total number of samples: 186\n",
      "The number of samples: 149\n",
      "The number of columns: 494\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 149 entries, Liver-030_0 to Liver-103_0\n",
      "Columns: 494 entries, label to PREDICT_original_phasef_phasesym_entropy_WL3_N5\n",
      "dtypes: float64(468), int64(25), object(1)\n",
      "memory usage: 576.2+ KB\n",
      "None\n",
      "categorical columns: 1\n",
      "numerical columns: 493\n",
      "samples with no variation: 0\n",
      "columns with no variation: 19\n",
      "Class Distribution: malignant label    76\n",
      "dtype: int64 and benign label    73\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### Installation of the necessary packages\n",
    "\n",
    "#general packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets as ds\n",
    "import seaborn\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#classifiers\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn import feature_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "#other imports\n",
    "from worcliver.load_data import load_data\n",
    "\n",
    "### Loading the data\n",
    "data = load_data()\n",
    "\n",
    "### Division into training and test\n",
    "amount_in_training = round(len(data.index)*0.8)\n",
    "amount_in_test = round(len(data.index)*0.2)\n",
    "\n",
    "#split the data into training data and test data\n",
    "training_data, test_data = model_selection.train_test_split(data, test_size=(amount_in_test) / len(data), random_state=42)\n",
    "\n",
    "#checking the lengths\n",
    "print(f'Training + validation: {len(training_data)} samples, Test: {len(test_data)} samples.')\n",
    "print(f'So total number of samples: {len(training_data)+len(test_data)}')\n",
    "\n",
    "### Data inspection: only on the training set!\n",
    "\n",
    "print(f'The number of samples: {len(training_data.index)}')\n",
    "print(f'The number of columns: {len(training_data.columns)}')\n",
    "\n",
    "training_data_df = pd.DataFrame(training_data)\n",
    "print(training_data_df.info())\n",
    "cat_cols = training_data_df.select_dtypes(include=[\"object\"]).columns\n",
    "num_cols = training_data_df.select_dtypes(include=[\"number\"]).columns\n",
    "print(f\"categorical columns: {len(cat_cols)}\")\n",
    "print(f'numerical columns: {len(num_cols)}')\n",
    "\n",
    "no_variation_samples = 0\n",
    "for sample in training_data.index:\n",
    "    data_sample = training_data.drop(columns=['label'])\n",
    "    if data_sample.loc[sample].max() == data_sample.loc[sample].min():\n",
    "        no_variation_samples += 1\n",
    "no_variation_col = 0\n",
    "for feature in training_data.columns:\n",
    "    if training_data[feature].max() == training_data[feature].min():\n",
    "        no_variation_col += 1\n",
    "print(f'samples with no variation: {no_variation_samples}')\n",
    "print(f'columns with no variation: {no_variation_col}')\n",
    "\n",
    "label = training_data[['label']]\n",
    "malignant_count = (label == \"malignant\").sum()\n",
    "benign_count = (label == \"benign\").sum()\n",
    "\n",
    "print(f'Class Distribution: malignant {malignant_count} and benign {benign_count}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdf40d",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The function definitions of the functions used for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "900c1773",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some functions for the preprocessing of the data, decisions made based on training set results\n",
    "\n",
    "def determining_labels(training_data):\n",
    "    '''function definition'''\n",
    "    label_training = training_data[['label']]\n",
    "    training_data.drop(columns=['label'], inplace=True)\n",
    "\n",
    "    label_training_binary = label_training.copy()\n",
    "    label_training_binary['label'] = label_training_binary['label'].map({'malignant': 1, 'benign': 0})\n",
    "    return label_training_binary\n",
    "\n",
    "def handling_missing_data(training_data):\n",
    "    '''function definition'''\n",
    "    data_training = pd.DataFrame(training_data)\n",
    "\n",
    "    #if missing data is stored as NaN, replace with the mean\n",
    "    nan_check = data_training.isna()\n",
    "    nan_counts = nan_check.sum()\n",
    "    missing_data_features = nan_counts[nan_counts > 0].index.tolist()\n",
    "\n",
    "    for feature in missing_data_features:\n",
    "        mean_value = data_training[feature].mean()\n",
    "        data_training[feature].fillna(mean_value, inplace=True)\n",
    "    \n",
    "    #if missing data is stored as 0, don't handle this as missing data: values in Radiomics can also be determined as 0, so \n",
    "    #it cannot be stated that 0 is a missing value. \n",
    "\n",
    "    return data_training\n",
    "\n",
    "def shapiro_wilk_testing(data_training):\n",
    "    '''function definition'''\n",
    "    shapiro_results = {}\n",
    "    no_variation = 0\n",
    "    features_no_variation = []\n",
    "\n",
    "    for feature in data_training.columns:\n",
    "        if data_training[feature].max() == data_training[feature].min():\n",
    "            shapiro_results[feature] = None  #skipping the feature when there is no variation\n",
    "            no_variation += 1\n",
    "            features_no_variation.append(feature)\n",
    "        else:\n",
    "            #Shapiro-Wilk test to check for normality\n",
    "            stat, p_value = stats.shapiro(data_training[feature])\n",
    "\n",
    "            #if normally distributed, store 1, else store 0\n",
    "            if p_value > 0.05:\n",
    "                result = 1\n",
    "            else:\n",
    "                result = 0\n",
    "\n",
    "            shapiro_results[feature] = result\n",
    "\n",
    "    return features_no_variation, shapiro_results\n",
    "\n",
    "def normal_distributed_data(shapiro_results): #only relevant on training data, this determines the next steps!\n",
    "    '''function definition'''\n",
    "    if sum(1 for result in shapiro_results.values() if result == 0) > 0:\n",
    "        normal_distributed = False\n",
    "    return normal_distributed\n",
    "    \n",
    "def delete_no_varation(data_training, features_no_variation):\n",
    "    '''function definition'''\n",
    "    data_training_clean = data_training.drop(columns=features_no_variation)\n",
    "    return data_training_clean\n",
    "\n",
    "def standardize_data(data_training_clean, normal_distributed):\n",
    "    '''function definition'''\n",
    "    if not normal_distributed:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        data_training_standardized = scaler.fit_transform(data_training_clean)\n",
    "    else:\n",
    "        data_training_standardized = data_training_clean\n",
    "\n",
    "    data_training_standardized_df = pd.DataFrame(data_training_standardized, columns=data_training_clean.columns)\n",
    "    return data_training_standardized_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddd8cf",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "The function definitions of the functions used for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69db9b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Univariate selection\n",
    "\n",
    "def univariate_feature_selection(data_training_normalized_df):\n",
    "    '''function definition'''\n",
    "    corr_matrix = data_training_normalized_df.corr()\n",
    "\n",
    "    #step 1: calculate the correlation and associated p-value for every feature-pair\n",
    "    corr_matrix = pd.DataFrame(index=data_training_normalized_df.columns, columns=data_training_normalized_df.columns, dtype=float)\n",
    "    pval_matrix = pd.DataFrame(index=data_training_normalized_df.columns, columns=data_training_normalized_df.columns, dtype=float)\n",
    "\n",
    "    for col1 in data_training_normalized_df.columns:\n",
    "        for col2 in data_training_normalized_df.columns:\n",
    "            if col1 == col2:\n",
    "                corr_matrix.loc[col1, col2] = 1.0\n",
    "                pval_matrix.loc[col1, col2] = 0.0\n",
    "            else:\n",
    "                r, p = stats.pearsonr(data_training_normalized_df[col1], data_training_normalized_df[col2])\n",
    "                corr_matrix.loc[col1, col2] = r\n",
    "                pval_matrix.loc[col1, col2] = p\n",
    "\n",
    "\n",
    "    #step 2: loop through all the feature pairs and determine which ones have a high correlation and low p-value\n",
    "    threshold = 0.9\n",
    "    p_threshold = 0.05\n",
    "\n",
    "    to_drop = set()\n",
    "\n",
    "    for i, col1 in enumerate(data_training_normalized_df.columns):\n",
    "        for j, col2 in enumerate(data_training_normalized_df.columns):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            r = corr_matrix.loc[col1, col2]\n",
    "            p = pval_matrix.loc[col1, col2]\n",
    "\n",
    "            if abs(r) > threshold and p < p_threshold:\n",
    "                if col1 not in to_drop and col2 not in to_drop:\n",
    "                    col_to_drop = col2\n",
    "                    to_drop.add(col_to_drop)\n",
    "\n",
    "    print(\"Dropped features due to high correlation:\")\n",
    "    print(to_drop)\n",
    "\n",
    "    #step 3: drop one of the features in a feature pair with high correlation and low p-value\n",
    "    data_training_reduced = data_training_normalized_df.drop(columns=to_drop)\n",
    "\n",
    "    print(f\"Size of new training data set is:{data_training_reduced.shape}; Old was: {data_training_normalized_df.shape}\")\n",
    "    return data_training_reduced\n",
    "\n",
    "\n",
    "def lasso_feature_selection(data_training_reduced, label_training_binary):\n",
    "    '''function definition'''\n",
    "    label_training_array = label_training_binary.values.flatten()\n",
    "\n",
    "    #step 1: perform Lasso with cross-validation for 50 values of alpha on 10 segments over 10.000 iterations\n",
    "    lasso_cv = LassoCV(alphas=np.logspace(-4, 0, 100), cv=10, max_iter=10000)  \n",
    "    lasso_cv.fit(data_training_reduced, label_training_array)\n",
    "\n",
    "    #step 2: find the best alpha value\n",
    "    best_alpha = lasso_cv.alpha_\n",
    "    print(f\"Optimal Alpha: {best_alpha}\")\n",
    "\n",
    "    #step 3: fit LASSO again with optimal alpha\n",
    "    lasso_opt = Lasso(alpha=best_alpha)\n",
    "    lasso_opt.fit(data_training_reduced, label_training_array)\n",
    "\n",
    "    #step 4: get the features of interest, and only extract those\n",
    "    selected_features = data_training_reduced.columns[lasso_opt.coef_ != 0]\n",
    "    print(f\"Number of selected features: {len(selected_features)}\")\n",
    "    print(f\"Final Selected Features: {list(selected_features)} \")\n",
    "\n",
    "    training_data_selected = data_training_reduced[selected_features]\n",
    "    training_data_selected_df = pd.DataFrame(training_data_selected)\n",
    "\n",
    "    print(f\"Size of new training data set is:{training_data_selected.shape}\")\n",
    "    return training_data_selected_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f3d81",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "\n",
    "### K-Nearest Neighbours\n",
    "Function definition used for the KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b2975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_classifier(training_data_selected_df, label_training_binary):\n",
    "\n",
    "    label_training_array = label_training_binary.values.flatten()\n",
    "    input_knn = training_data_selected_df\n",
    "\n",
    "    x_train_KNN = input_knn\n",
    "    y_train_KNN = label_training_array\n",
    "\n",
    "    param_grid = {\n",
    "        'n_neighbors': list(range(1, 31)) ,\n",
    "    }   \n",
    "\n",
    "    # Initialize the classifier\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    # GridSearchCV to find the best k\n",
    "    grid_search = model_selection.GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "    # Fit model\n",
    "    grid_search.fit(x_train_KNN, y_train_KNN)\n",
    "\n",
    "    # # Output results\n",
    "    # print(\"Best number of neighbors:\", grid_search.best_params_['n_neighbors'])\n",
    "    # print(\"Best cross-validation accuracy: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "    # Convert cv_results_ to a DataFrame for easier manipulation\n",
    "    results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "    # Vind alle individuele fold score kolommen\n",
    "    fold_columns = [col for col in results_df.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "\n",
    "    # Aantal folds\n",
    "    n_folds = len(fold_columns)\n",
    "\n",
    "    # Functie om CI te berekenen per rij\n",
    "    def compute_confidence_interval(row):\n",
    "        scores = row[fold_columns].values\n",
    "        #############################################vanaf hier gaat het mis, scores is leeg\n",
    "        mean = np.mean(scores)\n",
    "        std = np.std(scores, ddof=1)  # sample std dev\n",
    "        ci_halfwidth = stats.norm.ppf(0.975) * std / np.sqrt(n_folds)\n",
    "        return pd.Series({\n",
    "            'mean_score': mean,\n",
    "            'std_score': std,\n",
    "            'ci_lower': mean - ci_halfwidth,\n",
    "            'ci_upper': mean + ci_halfwidth\n",
    "        })\n",
    "\n",
    "    # Pas toe op elke rij (elke modelconfiguratie)\n",
    "    ci_df = results_df.apply(compute_confidence_interval, axis=1)\n",
    "    print(f'ci_df: {ci_df}')\n",
    "\n",
    "    # Build a DataFrame with all the relevant info\n",
    "    # final_df = pd.concat([\n",
    "    #     results_df[['params', 'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score']],\n",
    "    #     ci_df\n",
    "    # ], axis=1)\n",
    "\n",
    "    final_df = pd.concat([results_df['params'], ci_df], axis=1)\n",
    "\n",
    "    # Print de top 5 met hoogste mean_score\n",
    "    top_models = final_df.sort_values(by='mean_score', ascending=False).head(5)\n",
    "    print(top_models)\n",
    "\n",
    "    #retraining the model with the top parameters\n",
    "    top_params_KNN = top_models.iloc[0]['params']\n",
    "    final_KNN_model = KNeighborsClassifier(**top_params_KNN)\n",
    "    final_KNN_model.fit(x_train_KNN, y_train_KNN)\n",
    "\n",
    "    return final_KNN_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ede8bd",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "Function definition used for the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bcb5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_classifier(training_data_selected_df, label_training_binary):\n",
    "    label_training_array = label_training_binary.values.flatten()\n",
    "    input_random_forest = training_data_selected_df\n",
    "    \n",
    "    X_train_RF = input_random_forest\n",
    "    y_train_RF = label_training_array\n",
    "\n",
    "    param_grid_RF = {\n",
    "        'n_estimators': [110, 115, 120],                     #trees\n",
    "        'max_depth': [5, 8, 13],                         #maximum depth\n",
    "        'bootstrap': [True],                     #bootstrap on or off >> moet dus True of False zijn, maar aangezien er altijd True kwam dat nu even gelaten voor snelheid runnen\n",
    "        'min_samples_split': [2, 3, 4, 5],                    #minimum samples required to split\n",
    "        'min_samples_leaf': [1, 2, 3],                  #minimum samples wat je uiteindelijk in een leaf overhoudt \n",
    "        'max_features': ['sqrt', 'log2'],                       #controls the number of features selected, can also try log2 \n",
    "        'min_impurity_decrease': [0.00001, 0.000005, 0.00002]   #controls the minimum amount of impurity reduction required for a split to be created    \n",
    "    }\n",
    "\n",
    "    random_forest = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    #de grid search naar de beste hyperparameters\n",
    "    grid_search_RF = model_selection.GridSearchCV(random_forest, param_grid_RF, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    #grid_search = RandomizedSearchCV(random_forest, param_grid, cv=10, scoring='accuracy', n_iter=20, n_jobs=-1, random_state=42, verbose=1)\n",
    "\n",
    "    # Fit GridSearchCV to the data\n",
    "    grid_search_RF.fit(X_train_RF, y_train_RF)\n",
    "\n",
    "    #find the accuracies per fold\n",
    "    results_df_RF = pd.DataFrame(grid_search_RF.cv_results_)\n",
    "\n",
    "    # Vind alle individuele fold score kolommen\n",
    "    fold_columns_RF = [col for col in results_df_RF.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "\n",
    "    # Aantal folds\n",
    "    n_folds_RF = len(fold_columns_RF)\n",
    "\n",
    "    # Functie om CI te berekenen per rij\n",
    "    def compute_confidence_interval_RF(row):\n",
    "        scores = row[fold_columns_RF].values\n",
    "        mean = np.mean(scores)\n",
    "        std = np.std(scores, ddof=1)  # sample std dev\n",
    "        ci_halfwidth = stats.norm.ppf(0.975) * std / np.sqrt(n_folds_RF)\n",
    "        return pd.Series({\n",
    "            'mean_score': mean,\n",
    "            'std_score': std,\n",
    "            'ci_lower': mean - ci_halfwidth,\n",
    "            'ci_upper': mean + ci_halfwidth\n",
    "        })\n",
    "\n",
    "    # Pas toe op elke rij (elke modelconfiguratie)\n",
    "    ci_RF = results_df_RF.apply(compute_confidence_interval_RF, axis=1)\n",
    "\n",
    "    # Combineer met de parameterinstellingen\n",
    "    final_RF = pd.concat([results_df_RF['params'], ci_RF], axis=1)\n",
    "\n",
    "    # Print de top 5 met hoogste mean_score\n",
    "    top_models_RF = final_RF.sort_values(by='mean_score', ascending=False).head(5)\n",
    "\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(top_models_RF)\n",
    "\n",
    "    #retraining the model with the top parameters\n",
    "    top_params_RF = top_models_RF.iloc[0]['params']\n",
    "    final_RF_model = RandomForestClassifier(**top_params_RF, random_state=42)\n",
    "    final_RF_model.fit(X_train_RF, y_train_RF)\n",
    "\n",
    "    return final_RF_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1bd854",
   "metadata": {},
   "source": [
    "### Support vector machine\n",
    "Function definition used for the support vector machine classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15960596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_classifier(training_data_selected_df, label_training_binary):\n",
    "    label_training_array = label_training_binary.values.flatten()\n",
    "    input_SVM = training_data_selected_df\n",
    "\n",
    "    X_train_SVM = input_SVM\n",
    "    y_train_SVM = label_training_array\n",
    "\n",
    "    param_grid_SVM = {\n",
    "        'kernel': ['linear', 'poly', 'rbf'],        # kernel-types\n",
    "        'C': [0.01, 0.1, 1, 2, 3, 5, 10, 100],      # Expand the range of C values\n",
    "        'gamma': ['scale', 'auto', 0.01, 0.05, 0.1, 0.2, 0.5], # Add more gamma values\n",
    "        'degree': [1, 3, 5, 10, 12, 15],            # Add more degree values for polynomial kernel\n",
    "        'coef0': [0.0, 0.01, 0.5, 1, 2]             # Extend coef0 range\n",
    "    }\n",
    "\n",
    "    SVM_model = SVC()\n",
    "\n",
    "    #de grid search naar de beste hyperparameters\n",
    "    grid_search_SVM = model_selection.GridSearchCV(SVM_model, param_grid_SVM, cv=10, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    #grid_search = RandomizedSearchCV(random_forest, param_grid, cv=10, scoring='accuracy', n_iter=20, n_jobs=-1, random_state=42, verbose=1)\n",
    "\n",
    "    # Fit GridSearchCV to the data\n",
    "    grid_search_SVM.fit(X_train_SVM, y_train_SVM)\n",
    "\n",
    "    #find the accuracies per fold\n",
    "    results_df_SVM = pd.DataFrame(grid_search_SVM.cv_results_)\n",
    "\n",
    "    # Vind alle individuele fold score kolommen\n",
    "    fold_columns_SVM = [col for col in results_df_SVM.columns if col.startswith(\"split\") and col.endswith(\"_test_score\")]\n",
    "\n",
    "    # Aantal folds\n",
    "    n_folds_SVM = len(fold_columns_SVM)\n",
    "\n",
    "    # Functie om CI te berekenen per rij\n",
    "    def compute_confidence_interval_SVM(row):\n",
    "        scores = row[fold_columns_SVM].values\n",
    "        mean = np.mean(scores)\n",
    "        std = np.std(scores, ddof=1)  # sample std dev\n",
    "        ci_halfwidth = stats.norm.ppf(0.975) * std / np.sqrt(n_folds_SVM)\n",
    "        return pd.Series({\n",
    "            'mean_score': mean,\n",
    "            'std_score': std,\n",
    "            'ci_lower': mean - ci_halfwidth,\n",
    "            'ci_upper': mean + ci_halfwidth\n",
    "        })\n",
    "\n",
    "    # Pas toe op elke rij (elke modelconfiguratie)\n",
    "    ci_SVM = results_df_SVM.apply(compute_confidence_interval_SVM, axis=1)\n",
    "\n",
    "    # Combineer met de parameterinstellingen\n",
    "    final_SVM = pd.concat([results_df_SVM['params'], ci_SVM], axis=1)\n",
    "\n",
    "    # Print de top 5 met hoogste mean_score\n",
    "    top_models_SVM = final_SVM.sort_values(by='mean_score', ascending=False).head(5)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(top_models_SVM)\n",
    "\n",
    "    #retraining the model with the top parameters\n",
    "    top_params_SVM = top_models_SVM.iloc[0]['params']\n",
    "    final_SVM_model = SVC(**top_params_SVM)\n",
    "    final_SVM_model.fit(X_train_SVM, y_train_SVM)\n",
    "\n",
    "    return final_SVM_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a778952c",
   "metadata": {},
   "source": [
    "### Creating the machine learning models and testing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7196bf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped features due to high correlation:\n",
      "{'PREDICT_original_vf_Frangi_edge_peak_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_phasef_monogenic_peak_position_WL3_N5', 'PREDICT_original_tf_Gabor_peak_F0.5_A2.36', 'PREDICT_original_tf_GLCM_dissimilarityd3.0A0.0', 'PREDICT_original_tf_GLCM_energyd3.0A2.36', 'PREDICT_original_tf_GLCMMS_energyd1.0A0.79std', 'PREDICT_original_tf_Gabor_range_F0.2_A0.0', 'PREDICT_original_tf_GLCMMS_contrastd1.0A1.57mean', 'PREDICT_original_tf_GLCMMS_ASMd1.0A1.57mean', 'PREDICT_original_tf_Gabor_quartile_range_F0.5_A1.57', 'PREDICT_original_vf_Frangi_full_mean_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_GLCMMS_energyd1.0A0.0mean', 'PREDICT_original_tf_GLCMMS_dissimilarityd1.0A2.36mean', 'PREDICT_original_vf_Frangi_edge_skewness_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_LBP_range_R8_P24', 'PREDICT_original_tf_GLCM_homogeneityd1.0A2.36', 'PREDICT_original_tf_GLCMMS_homogeneityd1.0A0.79std', 'PREDICT_original_tf_GLCMMS_contrastd3.0A0.79std', 'PREDICT_original_tf_GLCMMS_dissimilarityd3.0A0.0mean', 'PREDICT_original_tf_Gabor_energy_F0.05_A0.79', 'PREDICT_original_tf_Gabor_std_F0.2_A0.79', 'PREDICT_original_tf_GLCMMS_correlationd3.0A0.79std', 'PREDICT_original_tf_GLCMMS_correlationd1.0A0.79mean', 'PREDICT_original_phasef_phasecong_kurtosis_WL3_N5', 'PREDICT_original_phasef_phasesym_quartile_range_WL3_N5', 'PREDICT_original_tf_GLCM_correlationd1.0A2.36', 'PREDICT_original_tf_GLCMMS_ASMd3.0A0.79std', 'PREDICT_original_tf_GLCMMS_dissimilarityd1.0A0.0std', 'PREDICT_original_tf_Gabor_entropy_F0.05_A1.57', 'PREDICT_original_tf_Gabor_entropy_F0.5_A1.57', 'PREDICT_original_tf_GLCMMS_homogeneityd3.0A1.57std', 'PREDICT_original_tf_Gabor_range_F0.05_A0.0', 'PREDICT_original_logf_kurtosis_sigma5', 'PREDICT_original_tf_GLCMMS_contrastd1.0A0.79std', 'PREDICT_original_tf_GLCMMS_homogeneityd1.0A0.79mean', 'PREDICT_original_tf_GLCM_energyd1.0A1.57', 'PREDICT_original_tf_GLCM_dissimilarityd1.0A1.57', 'PREDICT_original_tf_GLCM_energyd3.0A1.57', 'PREDICT_original_tf_GLCMMS_homogeneityd1.0A1.57std', 'PREDICT_original_tf_GLCM_energyd1.0A0.79', 'PREDICT_original_tf_GLCM_homogeneityd3.0A1.57', 'PREDICT_original_tf_GLCM_correlationd3.0A1.57', 'PREDICT_original_tf_LBP_entropy_R15_P36', 'PREDICT_original_tf_GLCMMS_correlationd1.0A1.57mean', 'PREDICT_original_tf_GLCM_correlationd1.0A1.57', 'PREDICT_original_tf_Gabor_quartile_range_F0.2_A0.79', 'PREDICT_original_tf_Gabor_entropy_F0.5_A0.0', 'PREDICT_original_tf_GLCMMS_correlationd1.0A2.36std', 'PREDICT_original_tf_GLCMMS_energyd1.0A1.57mean', 'PREDICT_original_tf_GLCM_correlationd1.0A0.0', 'PREDICT_original_tf_GLCM_energyd3.0A0.0', 'PREDICT_original_tf_GLCMMS_correlationd1.0A0.0std', 'PREDICT_original_tf_Gabor_std_F0.5_A0.79', 'PREDICT_original_vf_Frangi_inner_mean_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_LBP_entropy_R8_P24', 'PREDICT_original_tf_GLCMMS_dissimilarityd3.0A1.57mean', 'PREDICT_original_tf_GLCM_contrastd3.0A0.0', 'PREDICT_original_tf_GLCMMS_energyd1.0A1.57std', 'PREDICT_original_tf_Gabor_quartile_range_F0.5_A2.36', 'PREDICT_original_tf_GLCMMS_homogeneityd3.0A0.0std', 'PREDICT_original_tf_GLCMMS_correlationd1.0A1.57std', 'PREDICT_original_tf_LBP_quartile_range_R8_P24', 'PREDICT_original_tf_GLCM_contrastd3.0A2.36', 'PREDICT_original_tf_Gabor_min_F0.5_A1.57', 'PREDICT_original_tf_Gabor_entropy_F0.05_A2.36', 'PREDICT_original_tf_GLCMMS_contrastd1.0A0.0mean', 'PREDICT_original_phasef_phasecong_std_WL3_N5', 'PREDICT_original_sf_convexity_std_2.5D', 'PREDICT_original_tf_Gabor_min_F0.5_A2.36', 'PREDICT_original_tf_GLCMMS_homogeneityd3.0A0.79std', 'PREDICT_original_tf_GLCM_homogeneityd3.0A0.79', 'PREDICT_original_tf_GLCM_contrastd3.0A1.57', 'PREDICT_original_vf_Frangi_edge_max_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_vf_Frangi_inner_energy_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_vf_Frangi_edge_mean_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_phasef_phasesym_range_WL3_N5', 'PREDICT_original_tf_GLCM_energyd3.0A0.79', 'PREDICT_original_tf_Gabor_std_F0.05_A0.0', 'PREDICT_original_tf_GLCM_dissimilarityd1.0A0.0', 'PREDICT_original_tf_GLCMMS_dissimilarityd1.0A1.57mean', 'PREDICT_original_tf_GLCMMS_contrastd3.0A0.79mean', 'PREDICT_original_tf_Gabor_range_F0.2_A2.36', 'PREDICT_original_tf_GLCM_dissimilarityd1.0A2.36', 'PREDICT_original_tf_GLCMMS_contrastd3.0A1.57std', 'PREDICT_original_tf_LBP_entropy_R3_P12', 'PREDICT_original_tf_GLCM_contrastd3.0A0.79', 'PREDICT_original_tf_GLCMMS_correlationd3.0A0.0std', 'PREDICT_original_vf_Frangi_edge_kurtosis_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_GLCMMS_contrastd3.0A2.36mean', 'PREDICT_original_tf_GLCM_contrastd1.0A1.57', 'PREDICT_original_vf_Frangi_inner_kurtosis_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_Gabor_entropy_F0.2_A0.0', 'PREDICT_original_logf_quartile_range_sigma10', 'PREDICT_original_hf_range', 'PREDICT_original_tf_GLCMMS_contrastd3.0A2.36std', 'PREDICT_original_tf_Gabor_quartile_range_F0.2_A0.0', 'PREDICT_original_tf_GLCMMS_ASMd1.0A1.57std', 'PREDICT_original_tf_GLCMMS_ASMd3.0A2.36std', 'PREDICT_original_tf_Gabor_std_F0.5_A0.0', 'PREDICT_original_tf_GLCM_homogeneityd3.0A0.0', 'PREDICT_original_logf_std_sigma1', 'PREDICT_original_phasef_phasesym_std_WL3_N5', 'PREDICT_original_logf_max_sigma1', 'PREDICT_original_tf_GLCMMS_homogeneityd3.0A2.36std', 'PREDICT_original_tf_Gabor_max_F0.2_A1.57', 'PREDICT_original_phasef_monogenic_median_WL3_N5', 'PREDICT_original_tf_GLCMMS_contrastd1.0A1.57std', 'PREDICT_original_tf_GLCMMS_energyd3.0A1.57std', 'PREDICT_original_tf_GLCMMS_energyd3.0A2.36std', 'PREDICT_original_tf_GLCMMS_ASMd3.0A2.36mean', 'PREDICT_original_tf_GLCMMS_contrastd3.0A0.0std', 'PREDICT_original_tf_Gabor_std_F0.05_A0.79', 'PREDICT_original_tf_Gabor_mean_F0.5_A2.36', 'PREDICT_original_vf_Frangi_edge_range_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_GLCM_homogeneityd1.0A0.79', 'PREDICT_original_tf_GLCM_correlationd3.0A0.79', 'PREDICT_original_logf_entropy_sigma1', 'PREDICT_original_tf_GLCM_ASMd1.0A1.57', 'PREDICT_original_tf_Gabor_entropy_F0.2_A1.57', 'PREDICT_original_tf_Gabor_peak_F0.5_A0.0', 'PREDICT_original_tf_GLCMMS_contrastd1.0A0.0std', 'PREDICT_original_tf_LBP_kurtosis_R15_P36', 'PREDICT_original_tf_GLCM_ASMd3.0A0.0', 'PREDICT_original_tf_GLCM_dissimilarityd1.0A0.79', 'PREDICT_original_tf_GLCMMS_homogeneityd3.0A0.79mean', 'PREDICT_original_sf_solidity_std_2.5D', 'PREDICT_original_logf_entropy_sigma5', 'PREDICT_original_tf_GLCM_energyd1.0A2.36', 'PREDICT_original_tf_GLCMMS_ASMd3.0A0.0std', 'PREDICT_original_logf_peak_sigma1', 'PREDICT_original_vf_Frangi_edge_quartile_range_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_GLCMMS_ASMd3.0A1.57mean', 'PREDICT_original_vf_Frangi_edge_min_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_Gabor_entropy_F0.5_A0.79', 'PREDICT_original_tf_GLCMMS_correlationd1.0A0.0mean', 'PREDICT_original_tf_LBP_mean_R15_P36', 'PREDICT_original_tf_Gabor_std_F0.2_A2.36', 'PREDICT_original_tf_GLCM_homogeneityd3.0A2.36', 'PREDICT_original_tf_GLCMMS_correlationd3.0A0.79mean', 'PREDICT_original_tf_Gabor_entropy_F0.2_A0.79', 'PREDICT_original_phasef_monogenic_entropy_WL3_N5', 'PREDICT_original_logf_range_sigma1', 'PREDICT_original_tf_LBP_std_R15_P36', 'PREDICT_original_tf_Gabor_std_F0.05_A1.57', 'PREDICT_original_tf_Gabor_max_F0.5_A2.36', 'PREDICT_original_tf_GLCMMS_dissimilarityd3.0A0.0std', 'PREDICT_original_tf_GLCMMS_homogeneityd1.0A2.36mean', 'PREDICT_original_phasef_monogenic_skewness_WL3_N5', 'PREDICT_original_tf_GLCMMS_dissimilarityd1.0A1.57std', 'PREDICT_original_tf_Gabor_range_F0.05_A1.57', 'PREDICT_original_phasef_phasecong_quartile_range_WL3_N5', 'PREDICT_original_tf_Gabor_max_F0.5_A1.57', 'PREDICT_original_sf_area_std_2.5D', 'PREDICT_original_tf_GLCMMS_dissimilarityd3.0A2.36mean', 'PREDICT_original_tf_Gabor_quartile_range_F0.2_A2.36', 'PREDICT_original_tf_GLCM_energyd1.0A0.0', 'PREDICT_original_vf_Frangi_inner_range_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_phasef_phasecong_range_WL3_N5', 'PREDICT_original_sf_volume_2.5D', 'PREDICT_original_tf_GLCM_ASMd3.0A0.79', 'PREDICT_original_tf_Gabor_energy_F0.5_A2.36', 'PREDICT_original_logf_range_sigma5', 'PREDICT_original_tf_GLCMMS_homogeneityd1.0A0.0mean', 'PREDICT_original_logf_entropy_sigma10', 'PREDICT_original_tf_Gabor_range_F0.2_A1.57', 'PREDICT_original_tf_Gabor_entropy_F0.5_A2.36', 'PREDICT_original_tf_GLCMMS_ASMd1.0A0.79std', 'PREDICT_original_logf_kurtosis_sigma10', 'PREDICT_original_tf_GLCMMS_homogeneityd1.0A0.0std', 'PREDICT_original_tf_GLCMMS_dissimilarityd3.0A0.79std', 'PREDICT_original_tf_GLCMMS_energyd1.0A2.36std', 'PREDICT_original_tf_GLCMMS_ASMd1.0A0.79mean', 'PREDICT_original_vf_Frangi_full_range_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_GLCM_dissimilarityd3.0A1.57', 'PREDICT_original_tf_Gabor_range_F0.2_A0.79', 'PREDICT_original_tf_GLCMMS_dissimilarityd3.0A0.79mean', 'PREDICT_original_logf_peak_sigma5', 'PREDICT_original_tf_GLCMMS_energyd1.0A2.36mean', 'PREDICT_original_hf_median', 'PREDICT_original_tf_LBP_range_R15_P36', 'PREDICT_original_tf_Gabor_peak_F0.5_A0.79', 'PREDICT_original_tf_GLCM_contrastd1.0A0.79', 'PREDICT_original_tf_GLCM_contrastd1.0A2.36', 'PREDICT_original_tf_Gabor_entropy_F0.05_A0.0', 'PREDICT_original_phasef_phasesym_kurtosis_WL3_N5', 'PREDICT_original_hf_std', 'PREDICT_original_tf_LBP_median_R3_P12', 'PREDICT_original_tf_Gabor_range_F0.05_A2.36', 'PREDICT_original_tf_GLCMMS_energyd3.0A0.79mean', 'PREDICT_original_tf_Gabor_range_F0.5_A1.57', 'PREDICT_original_tf_Gabor_range_F0.5_A0.0', 'PREDICT_original_tf_GLCM_correlationd3.0A2.36', 'PREDICT_original_tf_GLCMMS_dissimilarityd1.0A0.79std', 'PREDICT_original_tf_Gabor_peak_F0.5_A1.57', 'PREDICT_original_tf_Gabor_std_F0.2_A1.57', 'PREDICT_original_phasef_monogenic_quartile_range_WL3_N5', 'PREDICT_original_tf_Gabor_std_F0.5_A2.36', 'PREDICT_original_tf_GLCMMS_dissimilarityd1.0A2.36std', 'PREDICT_original_vf_Frangi_inner_std_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_vf_Frangi_edge_median_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_GLCMMS_energyd3.0A1.57mean', 'PREDICT_original_tf_GLCMMS_energyd3.0A0.0std', 'PREDICT_original_tf_GLCMMS_ASMd1.0A2.36std', 'PREDICT_original_vf_Frangi_edge_energy_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_vf_Frangi_edge_entropy_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_GLCMMS_dissimilarityd1.0A0.0mean', 'PREDICT_original_tf_GLCMMS_ASMd1.0A0.0std', 'PREDICT_original_hf_entropy', 'PREDICT_original_tf_GLCMMS_ASMd3.0A1.57std', 'PREDICT_original_tf_GLCMMS_dissimilarityd1.0A0.79mean', 'PREDICT_original_tf_GLCM_ASMd1.0A2.36', 'PREDICT_original_vf_Frangi_inner_quartile_range_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_GLCMMS_contrastd1.0A2.36std', 'PREDICT_original_tf_LBP_skewness_R8_P24', 'PREDICT_original_tf_GLCM_ASMd3.0A2.36', 'PREDICT_original_tf_GLCMMS_energyd1.0A0.79mean', 'PREDICT_original_tf_Gabor_entropy_F0.05_A0.79', 'PREDICT_original_tf_LBP_skewness_R3_P12', 'PREDICT_original_hf_mean', 'PREDICT_original_tf_GLCMMS_contrastd1.0A0.79mean', 'PREDICT_original_tf_Gabor_range_F0.5_A2.36', 'PREDICT_original_tf_GLCMMS_homogeneityd1.0A1.57mean', 'PREDICT_original_tf_GLCMMS_correlationd1.0A0.79std', 'PREDICT_original_tf_GLCMMS_correlationd1.0A2.36mean', 'PREDICT_original_tf_GLCMMS_energyd3.0A0.0mean', 'PREDICT_original_tf_GLCMMS_energyd1.0A0.0std', 'PREDICT_original_vf_Frangi_inner_max_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_GLCM_dissimilarityd3.0A0.79', 'PREDICT_original_tf_Gabor_std_F0.05_A2.36', 'PREDICT_original_tf_GLCMMS_homogeneityd1.0A2.36std', 'PREDICT_original_phasef_monogenic_range_WL3_N5', 'PREDICT_original_tf_GLCMMS_energyd3.0A2.36mean', 'PREDICT_original_tf_Gabor_quartile_range_F0.2_A1.57', 'PREDICT_original_tf_GLCMMS_ASMd3.0A0.79mean', 'PREDICT_original_vf_Frangi_inner_min_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_Gabor_std_F0.2_A0.0', 'PREDICT_original_tf_Gabor_entropy_F0.2_A2.36', 'PREDICT_original_tf_GLCM_dissimilarityd3.0A2.36', 'PREDICT_original_tf_GLCMMS_ASMd1.0A2.36mean', 'PREDICT_original_tf_Gabor_max_F0.2_A0.0', 'PREDICT_original_tf_Gabor_max_F0.2_A0.79', 'PREDICT_original_tf_Gabor_range_F0.5_A0.79', 'PREDICT_original_vf_Frangi_edge_std_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_GLCM_ASMd1.0A0.79', 'PREDICT_original_tf_Gabor_range_F0.05_A0.79', 'PREDICT_original_vf_Frangi_full_kurtosis_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_GLCMMS_contrastd1.0A2.36mean', 'PREDICT_original_tf_GLCM_correlationd1.0A0.79', 'PREDICT_original_tf_GLCM_homogeneityd1.0A1.57', 'PREDICT_original_tf_GLCMMS_energyd3.0A0.79std', 'PREDICT_original_vf_Frangi_full_std_SR(1.0, 10.0)_SS2.0', 'PREDICT_original_tf_GLCM_ASMd3.0A1.57', 'PREDICT_original_logf_range_sigma10'}\n",
      "Size of new training data set is:(149, 221); Old was: (149, 474)\n",
      "Optimal Alpha: 0.04229242874389499\n",
      "Number of selected features: 26\n",
      "Final Selected Features: ['PREDICT_original_sf_prax_avg_2.5D', 'PREDICT_original_sf_area_max_2.5D', 'PREDICT_original_sf_area_min_2.5D', 'PREDICT_original_logf_mean_sigma1', 'PREDICT_original_logf_median_sigma1', 'PREDICT_original_logf_kurtosis_sigma1', 'PREDICT_original_logf_quartile_range_sigma1', 'PREDICT_original_logf_min_sigma5', 'PREDICT_original_logf_quartile_range_sigma5', 'PREDICT_original_logf_median_sigma10', 'PREDICT_original_logf_skewness_sigma10', 'PREDICT_original_logf_peak_position_sigma10', 'PREDICT_original_tf_LBP_std_R3_P12', 'PREDICT_original_tf_LBP_kurtosis_R8_P24', 'PREDICT_original_tf_GLCMMS_ASMd3.0A0.0mean', 'PREDICT_original_tf_GLCMMS_correlationd3.0A1.57std', 'PREDICT_original_tf_GLCMMS_correlationd3.0A2.36mean', 'PREDICT_original_tf_Gabor_min_F0.05_A2.36', 'PREDICT_original_tf_Gabor_peak_F0.05_A2.36', 'PREDICT_original_tf_Gabor_skewness_F0.2_A0.0', 'PREDICT_original_tf_Gabor_kurtosis_F0.2_A0.0', 'PREDICT_original_tf_Gabor_skewness_F0.2_A0.79', 'PREDICT_original_tf_Gabor_peak_F0.2_A1.57', 'PREDICT_original_tf_Gabor_skewness_F0.5_A0.0', 'PREDICT_original_tf_Gabor_kurtosis_F0.5_A1.57', 'PREDICT_original_phasef_phasecong_entropy_WL3_N5'] \n",
      "Size of new training data set is:(149, 26)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Loading the data\n",
    "data = load_data()\n",
    "\n",
    "### Division into training and test\n",
    "amount_in_training = round(len(data.index)*0.8)\n",
    "amount_in_test = round(len(data.index)*0.2)\n",
    "\n",
    "#split the data into training data and test data\n",
    "training_data, test_data = model_selection.train_test_split(data, test_size=(amount_in_test) / len(data), random_state=42)\n",
    "\n",
    "### Preprocessing the training data\n",
    "label_training_binary = determining_labels(training_data)\n",
    "data_training = handling_missing_data(training_data)\n",
    "features_no_variation, shapiro_results = shapiro_wilk_testing(data_training)\n",
    "normal_distributed = normal_distributed_data(shapiro_results)\n",
    "data_training_clean = delete_no_varation(data_training, features_no_variation)\n",
    "data_training_normalized_df = standardize_data(data_training_clean, normal_distributed)\n",
    "\n",
    "### Preprocessing the test data\n",
    "label_test_binary = determining_labels(test_data)\n",
    "data_test = handling_missing_data(test_data)\n",
    "data_test_clean = delete_no_varation(data_test, features_no_variation)\n",
    "data_test_normalized_df = standardize_data(data_test_clean, normal_distributed)\n",
    "\n",
    "### Preprocessing all the data together (needed for learning curves)\n",
    "label_test_binary = determining_labels(data)\n",
    "data = handling_missing_data(data)\n",
    "data_clean = delete_no_varation(data, features_no_variation)\n",
    "data_normalized_df = standardize_data(data_clean, normal_distributed)\n",
    "\n",
    "### Feature selection\n",
    "data_training_univariate = univariate_feature_selection(data_training_normalized_df)\n",
    "training_data_selected_df = lasso_feature_selection(data_training_univariate, label_training_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d79b91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\evade\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "scores:[nan nan nan nan nan nan nan nan nan nan]\n",
      "ci_df:     mean_score  std_score  ci_lower  ci_upper\n",
      "0          NaN        NaN       NaN       NaN\n",
      "1          NaN        NaN       NaN       NaN\n",
      "2          NaN        NaN       NaN       NaN\n",
      "3          NaN        NaN       NaN       NaN\n",
      "4          NaN        NaN       NaN       NaN\n",
      "5          NaN        NaN       NaN       NaN\n",
      "6          NaN        NaN       NaN       NaN\n",
      "7          NaN        NaN       NaN       NaN\n",
      "8          NaN        NaN       NaN       NaN\n",
      "9          NaN        NaN       NaN       NaN\n",
      "10         NaN        NaN       NaN       NaN\n",
      "11         NaN        NaN       NaN       NaN\n",
      "12         NaN        NaN       NaN       NaN\n",
      "13         NaN        NaN       NaN       NaN\n",
      "14         NaN        NaN       NaN       NaN\n",
      "15         NaN        NaN       NaN       NaN\n",
      "16         NaN        NaN       NaN       NaN\n",
      "17         NaN        NaN       NaN       NaN\n",
      "18         NaN        NaN       NaN       NaN\n",
      "19         NaN        NaN       NaN       NaN\n",
      "20         NaN        NaN       NaN       NaN\n",
      "21         NaN        NaN       NaN       NaN\n",
      "22         NaN        NaN       NaN       NaN\n",
      "23         NaN        NaN       NaN       NaN\n",
      "24         NaN        NaN       NaN       NaN\n",
      "25         NaN        NaN       NaN       NaN\n",
      "26         NaN        NaN       NaN       NaN\n",
      "27         NaN        NaN       NaN       NaN\n",
      "28         NaN        NaN       NaN       NaN\n",
      "29         NaN        NaN       NaN       NaN\n",
      "               params  mean_score  std_score  ci_lower  ci_upper\n",
      "0  {'n_neighbors': 1}         NaN        NaN       NaN       NaN\n",
      "1  {'n_neighbors': 2}         NaN        NaN       NaN       NaN\n",
      "2  {'n_neighbors': 3}         NaN        NaN       NaN       NaN\n",
      "3  {'n_neighbors': 4}         NaN        NaN       NaN       NaN\n",
      "4  {'n_neighbors': 5}         NaN        NaN       NaN       NaN\n",
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m### Training the classifiers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m final_KNN_model \u001b[38;5;241m=\u001b[39m KNN_classifier(training_data_selected_df, label_training_binary)\n\u001b[1;32m----> 3\u001b[0m final_RF_model \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_forest_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data_selected_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_training_binary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m final_SVM_model \u001b[38;5;241m=\u001b[39m SVM_classifier(training_data_selected_df, label_training_binary)\n",
      "Cell \u001b[1;32mIn[32], line 25\u001b[0m, in \u001b[0;36mrandom_forest_classifier\u001b[1;34m(training_data_selected_df, label_training_binary)\u001b[0m\n\u001b[0;32m     21\u001b[0m grid_search_RF \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mGridSearchCV(random_forest, param_grid_RF, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#grid_search = RandomizedSearchCV(random_forest, param_grid, cv=10, scoring='accuracy', n_iter=20, n_jobs=-1, random_state=42, verbose=1)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Fit GridSearchCV to the data\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43mgrid_search_RF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_RF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_RF\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#find the accuracies per fold\u001b[39;00m\n\u001b[0;32m     28\u001b[0m results_df_RF \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(grid_search_RF\u001b[38;5;241m.\u001b[39mcv_results_)\n",
      "File \u001b[1;32mc:\\Users\\evade\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\evade\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\evade\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\evade\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\evade\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\evade\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1061\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1061\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\evade\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:938\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 938\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    940\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\evade\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\evade\\anaconda3\\lib\\concurrent\\futures\\_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\evade\\anaconda3\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Training the classifiers\n",
    "final_KNN_model = KNN_classifier(training_data_selected_df, label_training_binary)\n",
    "final_RF_model = random_forest_classifier(training_data_selected_df, label_training_binary)\n",
    "final_SVM_model = SVM_classifier(training_data_selected_df, label_training_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4787cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of RF: 0.7297\n",
      "Test Accuracy of SVM: 0.8378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\evade\\anaconda3\\lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#accuracy of the prediction of the test set\n",
    "y_test = label_test_binary.values.flatten()\n",
    "X_test = data_test_normalized_df.reset_index(drop=True)[training_data_selected_df.columns]\n",
    "\n",
    "y_pred_KNN = final_KNN_model.predict(X_test)\n",
    "test_accuracy_KNN = accuracy_score(y_test, y_pred_KNN)\n",
    "print(f\"Test Accuracy of KNN: {test_accuracy_KNN:.4f}\")\n",
    "\n",
    "y_pred_RF = final_RF_model.predict(X_test)\n",
    "test_accuracy_RF = accuracy_score(y_test, y_pred_RF)\n",
    "print(f\"Test Accuracy of RF: {test_accuracy_RF:.4f}\")\n",
    "\n",
    "y_pred_SVM = final_SVM_model.predict(X_test)\n",
    "test_accuracy_SVM = accuracy_score(y_test, y_pred_SVM)\n",
    "print(f\"Test Accuracy of SVM: {test_accuracy_SVM:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
